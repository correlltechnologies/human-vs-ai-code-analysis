# Human vs AI Code Analysis

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Dataset](https://img.shields.io/badge/dataset-HuggingFace-yellow.svg)
![Status](https://img.shields.io/badge/status-active-brightgreen.svg)

An empirical research project analyzing stylistic, structural, and semantic differences between human-written code and AI-generated code using the OSS-forge/HumanVsAICode dataset. This project implements a comprehensive multi-stage analysis pipeline combining traditional code metrics, cyclomatic complexity analysis, and modern embedding-based visualizations.

## Overview

This repository provides a reproducible framework for comparing code authorship patterns across human developers and multiple AI code generation models (ChatGPT, DeepSeek Coder, and Qwen). The analysis leverages both traditional software engineering metrics and modern machine learning techniques to identify distinguishing characteristics.

## Features

- **Structural Metrics Extraction**: Automated extraction of code complexity metrics including:
  - Lines of code (LOC)
  - Average line length
  - Comment density
  - Indentation depth patterns
  - Function count and distribution
  - Variable naming conventions
  - Cyclomatic complexity (via radon)
  - Normalized complexity scores

- **Semantic Embeddings**: Generate high-quality code embeddings using OpenAI's text-embedding-3-small model for capturing semantic patterns beyond surface-level syntax

- **Visualization Suite**: Comprehensive visualization tools including:
  - Statistical comparison plots (boxplots, distributions)
  - UMAP dimensionality reduction for embedding visualization
  - Human vs AI clustering analysis
  - Model-specific pattern identification

- **Performance Optimized**: CPU-optimized parallel processing for large-scale dataset analysis

## Project Structure

```
human-vs-ai-code-analysis/
├── 01_extract_features.py      # Extract structural & complexity metrics
├── 02_generate_embeddings.py   # Generate OpenAI embeddings
├── 03_visualize_structure.py   # Visualize structural features
├── 04_visualize_umap.py        # Create UMAP visualizations
├── requirements.txt             # Python dependencies
├── results/                     # Output directory (auto-created)
│   ├── structural_features.csv
│   ├── embeddings.npy
│   └── *.png (visualization outputs)
└── README.md
```

## Installation

### Prerequisites

- Python 3.8 or higher
- OpenAI API key (for embedding generation)

### Setup

1. Clone this repository:
```bash
git clone https://github.com/yourusername/human-vs-ai-code-analysis.git
cd human-vs-ai-code-analysis
```

2. Create and activate a virtual environment (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install required dependencies:
```bash
pip install -r requirements.txt
```

4. Set up OpenAI API credentials:
```bash
# Create a .env file in the project root
echo "OPENAI_API_KEY=your_api_key_here" > .env
```

## Usage

The analysis pipeline consists of four sequential stages. Run each script in order:

### Stage 1: Feature Extraction
Extract structural and complexity metrics from the dataset:
```bash
python 01_extract_features.py
```
**Output**: `results/structural_features.csv`

### Stage 2: Generate Embeddings
Generate semantic embeddings using OpenAI's API:
```bash
python 02_generate_embeddings.py
```
**Output**: `results/embeddings.npy`, `results/embeddings_with_features.csv`

**Note**: This stage requires an OpenAI API key and will incur API costs based on usage.

### Stage 3: Visualize Structural Features
Create statistical visualizations of code metrics:
```bash
python 03_visualize_structure.py
```
**Output**: Distribution plots and comparison charts in `results/`

### Stage 4: UMAP Visualization
Generate dimensionality reduction visualizations of code embeddings:
```bash
python 04_visualize_umap.py
```
**Output**: UMAP cluster visualizations in `results/`

## Dataset

This project uses the [OSS-forge/HumanVsAICode](https://huggingface.co/datasets/OSS-forge/HumanVsAICode) dataset from Hugging Face, which contains:

- **Human-written code**: Real-world Python code from open-source projects
- **AI-generated code**: Equivalent implementations generated by:
  - ChatGPT (GPT-3.5/4)
  - DeepSeek Coder
  - Qwen Coder

Each sample includes the code snippet and authorship label, enabling direct comparison of coding patterns across different authorship types.

## Metrics Computed

### Structural Metrics
- **LOC** (Lines of Code): Total line count
- **Average Line Length**: Mean character count per line
- **Comment Density**: Ratio of comment lines to total lines
- **Indentation Depth**: Average indentation level
- **Function Count**: Number of function definitions
- **Variable Name Length**: Average identifier length
- **Cyclomatic Complexity**: Code complexity score per function
- **Normalized Scores**: Complexity and functions per 100 LOC

### Semantic Analysis
- **256-dimensional embeddings** from OpenAI's text-embedding-3-small
- **UMAP projections** for 2D visualization of semantic clusters

## Requirements

Core dependencies (see `requirements.txt` for complete list):
- `datasets` - Hugging Face datasets library
- `radon` - Python code complexity analyzer
- `matplotlib`, `seaborn` - Visualization
- `pandas`, `numpy`, `scipy` - Data analysis
- `transformers`, `torch` - ML frameworks
- `scikit-learn` - Machine learning utilities
- `umap-learn` - Dimensionality reduction
- `openai` - OpenAI API client
- `tqdm` - Progress bars
- `psutil` - CPU optimization
- `joblib` - Parallel processing

## Results & Outputs

All analysis outputs are saved to the `results/` directory:
- **CSV files**: Structured data with metrics and embeddings
- **PNG visualizations**: Publication-ready plots and charts
- **NPY arrays**: Embedding vectors for further analysis

## Performance Notes

- Feature extraction is CPU-optimized using all available cores
- Embedding generation uses batched API calls with rate limit handling
- Expected runtime varies based on dataset size and hardware
- Large datasets may require 1-2 hours for complete pipeline execution

## Future Extensions

- [ ] Train human vs AI classifier (TF-IDF + Logistic Regression)
- [ ] Advanced embedding techniques (CodeBERT, GraphCodeBERT)
- [ ] Runtime performance benchmarking
- [ ] Cross-language analysis (extend beyond Python)
- [ ] Temporal analysis of AI model evolution
- [ ] Fine-grained style transfer detection

## Contributing

Contributions are welcome! Please feel free to submit issues, fork the repository, and create pull requests for:
- Additional metrics or visualizations
- Support for other programming languages
- Performance optimizations
- Documentation improvements

## Citation

If you use this code or analysis in your research, please cite:

```bibtex
@misc{human-vs-ai-code-analysis,
  author = {Your Name},
  title = {Human vs AI Code Analysis: An Empirical Study},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/yourusername/human-vs-ai-code-analysis}
}
```

Dataset citation:
```bibtex
@dataset{humanvsaicode,
  title = {HumanVsAICode Dataset},
  author = {OSS-forge},
  year = {2024},
  publisher = {Hugging Face},
  url = {https://huggingface.co/datasets/OSS-forge/HumanVsAICode}
}
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- **OSS-forge** for providing the HumanVsAICode dataset
- **OpenAI** for the embedding API
- **Hugging Face** for the datasets infrastructure
- All contributors to the open-source libraries used in this project

---

**Disclaimer**: This is an academic research project intended for educational purposes and empirical analysis of code generation patterns. Results should be interpreted in context with appropriate statistical rigor.
